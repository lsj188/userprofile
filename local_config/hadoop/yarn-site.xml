<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
    <!--如果使用yarn管理资源，所有的分配单元将以Container容器为最小单位 -->
    <!--NodeManager总的内存及vcores配置start -->
    <property>
        <!--参数解释：NodeManager总的可用物理内存。注意，该参数是不可修改的，一旦设置，整个运行过程中不 可动态修改。
		另外，该参数的默认值是8192MB，即使你的机器内存不够8192MB，YARN也会按照这些内存来使用，因此，这个值通过一 定要配置。
		不过，Apache已经正在尝试将该参数做成可动态修改的。默认值：8192(8GB)当该值配置小于1024(1GB)时，
		NM是无法启动的！会>报错:NodeManager from  slavenode2 doesn't satisfy minimum allocations, Sending SHUTDOWN signal to the NodeManager.
		报错时不一定是内存参数问题，也有可能是cpu-vcores配置得有问题，需要修改yarn.nodemanager.resource.memory-mb及yarn.nodemanager.resource.cpu-vcores
		不起作用可修改 yarn-env.sh 中 JAVA_HEAP_MAX=-Xmx3072m 改为3G
		不可动态修改-->
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
    </property>
	
	<!--默认值：8-->
	<property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>4</value>
    </property>
	
	<!--默认值：2.1-->
	<property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
    </property>
	<!-- NodeManager总的内存及vcores配置end -->
	
    <!--Container内存及cpu-vcores大小配置start-->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>100</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>512</value>
    </property>
	<property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>4</value>
    </property>
    <!--Container内存及cpu-vcores大小配置end-->

    <!--调度器配置，FairSchedulerstart
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    </property>
    <property>
        <name>yarn.scheduler.fair.allocation.file</name>
        <value>/home/cluster/conf/hadoop/fair-scheduler.xml</value>
    </property>
    调度器配置end-->
	
    <!--日志聚合功能start-->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
        <description>Where to aggregate logs to.</description>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>86400</value>
    </property>
    <!--日志聚合功能end-->

    <!--开启jobhistory服务-->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://localhost:19888/jobhistory/logs/</value>
    </property>

    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

</configuration>
